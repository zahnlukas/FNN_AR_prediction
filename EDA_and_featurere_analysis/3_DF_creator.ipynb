{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from utils import *\n",
    "from customer_features import * \n",
    "from reminders_features import *\n",
    "from clarification_features import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creator file\n",
    "\n",
    "The 5 datasets evaluated in the study are created in this file using functions defined in utils,customer_features, clarification_features and reminders_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating dataset 1 & 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/invoices_basic.csv')\n",
    "df = preprocessing(df)\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "#save csv\n",
    "df.to_csv(\"data/dataset_1_and_2.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset 3: Invoice + Customer features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/invoices_basic.csv')\n",
    "\n",
    "df = preprocessing(df)\n",
    "\n",
    "# Calculate: near_payment_term_count; near_payment_term_ratio and overdue_ratio\n",
    "print(\"Calculating Payment Behaviour features\")\n",
    "df = payment_behaviour(df, 6, 4) # 6 months lookback; Overdue defined as +/- 4 days from due date\n",
    "\n",
    "# Calculate: rolling_avg_dso\n",
    "print(\"Calculating Rolling avg\")\n",
    "df = calculate_rolling_avg(df, 6) # in bast 6 months\n",
    "\n",
    "# Calculate: outstanding_invoices, paid_invoices, issued_invoices\n",
    "print(\"Calculating Outstanding invoices\")\n",
    "df = calculate_outstanding_invoices(df, 4) # 4 months = 120 days; Since all invoices with DSO and payment terms >120 were removed\n",
    "\n",
    "# Calculate ratio outstanding from issued_invoices and outstanding_invoices\n",
    "df = ratio_outstanding(df)\n",
    "\n",
    "# Bin count variables\n",
    "df = binning_counts(df)\n",
    "\n",
    "# Binarize count variables\n",
    "df = binarize_counts(df)\n",
    "\n",
    "# Extract day/month/year of due date and day of week\n",
    "df = extract_datetime_features(df)\n",
    "\n",
    "\n",
    "df.to_csv(\"data/dataset_3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Dataset 4\n",
    "\n",
    "- Inputs required: Dataset 3 and Reminders dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create subset of reminders that contains only invoices that are in main data\n",
    "\n",
    "df_main = pd.read_csv('data/dataset_3.csv')\n",
    "df_reminders = pd.read_csv(\"data/reminders.csv\")\n",
    "\n",
    "#Get unique customer ids from main data\n",
    "unique_customer_ids_main = df_main['customer_id'].unique()\n",
    "\n",
    "# Get list of customers in reminders that are also in main data\n",
    "mask_unique_customers = df_reminders['customer_id'].isin(unique_customer_ids_main)\n",
    "\n",
    "#Filter reminders\n",
    "df_reminders = df_reminders[mask_unique_customers]\n",
    "\n",
    "#Save df\n",
    "df_reminders.to_csv(\"data/reminders_subset.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reminders = pd.read_csv(\"data/reminders_subset.csv\")\n",
    "df_main = pd.read_csv('data/dataset_3.csv')\n",
    "\n",
    "# Make sure all dates are in datetime format\n",
    "df_main['receipt_date']=pd.to_datetime(df_main['receipt_date'],format='%Y-%m-%d')\n",
    "df_main['weighted_payment_date']=pd.to_datetime(df_main['weighted_payment_date'],format='%Y-%m-%d')\n",
    "df_main['due_date']=pd.to_datetime(df_main['due_date'],format='%Y-%m-%d')\n",
    "df_reminders['reminder_date'] = pd.to_datetime(df_reminders['reminder_date'])\n",
    "\n",
    "\n",
    "print(\"Calculating feature 1: \")\n",
    "df = count_past_reminders(df_main, df_reminders, 6)\n",
    "\n",
    "print(\"Calculating feature 2: \")\n",
    "df = average_reminder_stage(df_main, df_reminders, 6)\n",
    "\n",
    "print(\"Binning\")\n",
    "df = reminders_binning(df)\n",
    "\n",
    "print(\"Done\")\n",
    "\n",
    "#df.to_csv(\"data/dataset_4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clarifications\n",
    "\n",
    "Clarification featues added: \n",
    "- Count_past_clarifications\n",
    "- Count_dunning_stop\n",
    "- Avg_clarification_days\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_df = pd.read_csv(\"data/dataset_4.csv\")\n",
    "df_clarification = pd.read_csv('data/clarifications.csv')\n",
    "df_reminders = pd.read_csv('data/reminders_subset.csv')\n",
    "\n",
    "# Make sure all dates are in datetime format\n",
    "df_clarification['created_at']=pd.to_datetime(df_clarification['created_at'],format='%Y-%m-%d %H:%M:%S.%f')\n",
    "df_clarification['resolved_at']=pd.to_datetime(df_clarification['resolved_at'],format=\"%Y-%m-%d %H:%M:%S.%f\")\n",
    "main_df['receipt_date']=pd.to_datetime(main_df['receipt_date'],format='%Y-%m-%d')\n",
    "main_df['weighted_payment_date']=pd.to_datetime(main_df['weighted_payment_date'],format='%Y-%m-%d')\n",
    "main_df['due_date']=pd.to_datetime(main_df['due_date'],format='%Y-%m-%d')\n",
    "\n",
    "# To see how many change after cleaning\n",
    "print(\"Number of clarifications before cleaning: \", len(df_clarification))\n",
    "\n",
    "\n",
    "    # Cleaning #\n",
    "\n",
    "# Claning: all rows created after 01.05.2024 are dropped & all rows with missing values for \"resolved_at\" (cleaning like main data)\n",
    "df_clarification = df_clarification[df_clarification['created_at'] <= '2024-05-01']\n",
    "\n",
    "#Drop clarifications that are still open; clarifications that are not yet resolved\n",
    "df_clarification = df_clarification.dropna(subset=['resolved_at'])\n",
    "\n",
    "\n",
    "    # Merge with reminders to get customer_id:\n",
    "\n",
    "# Get customer_id for each clarification\n",
    "df_clarification = df_clarification.merge(df_reminders[['journal_entry_id', 'customer_id']], left_on='item_id', right_on='journal_entry_id', how='left')\n",
    "\n",
    "# percentage of rows with missing value for journal_entry_id\n",
    "print(\"Percentage of rows with missing value for journal_entry_id: \", df_clarification['journal_entry_id'].isnull().sum() / len(df_clarification))\n",
    "\n",
    "# Drop rows with missing value for journal_entry_id since cannot assign to customer\n",
    "df_clarification = df_clarification.dropna(subset=['journal_entry_id'])\n",
    "\n",
    "# Drop column journal_entry_id since item_id is the same\n",
    "df_clarification = df_clarification.drop(columns=['journal_entry_id'])\n",
    "\n",
    "\n",
    "    #Creation of clarification duration; defined as days between created_at and resolved_at\n",
    "    \n",
    "df_clarification[\"duration\"] = (df_clarification[\"resolved_at\"] - df_clarification[\"created_at\"]).dt.days\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_clarification = df_clarification.drop(columns=['restart_count', 'restart_date'])\n",
    "\n",
    "print(\"Number of clarifications after cleaning: \", len(df_clarification))\n",
    "\n",
    "\n",
    "    # Creation clarification features\n",
    "df = clarification_features(main_df, df_clarification, 6)\n",
    "\n",
    "# Bin clarification features since distribution heavily skewed especially for counts (most entries in df don't have clarifications, so value = 0)\n",
    "df = binned_clarification_features(df)\n",
    "\n",
    "\n",
    "#df.to_csv(\"data/dataset_5.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
