{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data splits\n",
    "\n",
    "In this notebook the datasets are split into Train/Test and validation data:\n",
    "\n",
    "- Split: 80/10/10\n",
    "- Scaling using X_train for min and max values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from feature_scaler import *\n",
    "\n",
    "df = pd.read_csv('EDA_and_feature_analysis/Data/Dataset_5.csv')\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### One hot encoding\n",
    "\n",
    "# Select features to encode\n",
    "cat_to_encode = [\"day_of_week\"]\n",
    "\n",
    "df = pd.get_dummies(df, columns=cat_to_encode)\n",
    "\n",
    "# Turn bools into ints\n",
    "for col in df.columns:\n",
    "    if df[col].dtype == 'bool':\n",
    "        df[col] = df[col].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"dso\"]\n",
    "\n",
    "features = [#keep for operations in this notebook then drop: \n",
    "            \"customer_id\", \"weighted_payment_date\",\n",
    "       # Dataset 1:\n",
    "       'payment_terms', 'customer_id_enc', 'log_amount', #  Feature            \n",
    "       'day_of_week_0', # day_of_due_date \n",
    "       'day_of_week_1', 'day_of_week_2', 'day_of_week_3', 'day_of_week_4',\n",
    "       'day_of_week_5', 'day_of_week_6',\n",
    "       # Dataset 2:\n",
    "       'country_enc',\n",
    "       # Dataset 3: payment features\n",
    "       'ratio_outstanding', \"near_payment_term_ratio\", \"overdue_ratio\", \"rolling_avg_dso\", \n",
    "       'paid_invoices',\"outstanding_invoices\" , \"near_payment_term_count\",\n",
    "       # Dataset 4: Reminder features\n",
    "       \"binary_reminder_count\" , \"average_reminder_stage\",\n",
    "       # Dataset 5: Clarification features\n",
    "       \"binary_count_past_clarifications\" , \"log_avg_clarification_days\", \"binary_dunning_stop\" \n",
    "       ]\n",
    "\n",
    "dataset = 5  # Select which dataset we can to create the split for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reset index\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Sort the DataFrame by date\n",
    "df = df.sort_values('weighted_payment_date')\n",
    "\n",
    "# Determine the split index\n",
    "split_index = int(len(df) * 0.8)\n",
    "\n",
    "# Find the payment date at split_index\n",
    "date_train_split = df.iloc[split_index]['weighted_payment_date']\n",
    "\n",
    "# Splitting main and side data based on date_train_split; side_data used for validation and test\n",
    "Train = df[df['weighted_payment_date'] <= date_train_split]\n",
    "side_data = df[df['weighted_payment_date'] > date_train_split]\n",
    "\n",
    "#Test if it worked\n",
    "print(\"Total invoice data shape: \", df.shape)\n",
    "print(\"Train_data shape: \", Train.shape)\n",
    "print(\"Test_data shape: \", side_data.shape)\n",
    "print(\"-------------------------\")\n",
    "print(\"Test if it worked:\") #printing the earliest and latest dates in the train and test sets\n",
    "print(\"Earliest date in train_data: \", Train['weighted_payment_date'].min())\n",
    "print(\"Latest date in train_data: \", Train['weighted_payment_date'].max())\n",
    "print(\"Earliest date in side_data: \", side_data['weighted_payment_date'].min())\n",
    "print(\"Latest date in side_data: \", side_data['weighted_payment_date'].max())\n",
    "print(\"-------------------------\")\n",
    "\n",
    "\n",
    "# Determine the split index for validation and test data (50% of 20% = 10/10)\n",
    "split_index_test = int(len(side_data) * 0.5)\n",
    "\n",
    "# Find the payment date at split_index\n",
    "date_val_split = side_data.iloc[split_index_test]['weighted_payment_date']\n",
    "\n",
    "# Splitting main and side data based on date_val_split\n",
    "Val = side_data[side_data['weighted_payment_date'] <= date_val_split]\n",
    "test_data = side_data[side_data['weighted_payment_date'] > date_val_split]\n",
    "\n",
    "\n",
    "## Create subset of val and test data with only the customers in train data\n",
    "\n",
    "#Get unique customer ids of train\n",
    "unique_customer_ids_train = Train['customer_id'].unique()\n",
    "\n",
    "mask_val_data = Val['customer_id'].isin(unique_customer_ids_train)\n",
    "mask_test_data = test_data['customer_id'].isin(unique_customer_ids_train)\n",
    "\n",
    "Val_subset = Val[mask_val_data]\n",
    "Test_subset = test_data[mask_test_data]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"feature_scaler(features_to_scale, X_train, X_val, X_test, scaler)\"\"\"\n",
    "\n",
    "#features to scale\n",
    "to_scale = [\"log_amount\"]\n",
    "\n",
    "# Call the feature_scaler function\n",
    "Train, Val_subset, Test_subset, fitted_scaler = feature_scaler(to_scale, Train, Val_subset, Test_subset)\n",
    "\n",
    "\n",
    "X_train = Train[features]\n",
    "y_train = Train[target]\n",
    "\n",
    "X_val_subset = Val_subset[features]\n",
    "y_val_subset = Val_subset[target]\n",
    "\n",
    "X_val = Val[features]\n",
    "y_val = Val[target]\n",
    "\n",
    "X_test_subset = Test_subset[features]\n",
    "y_test_subset = Test_subset[target]\n",
    "\n",
    "X_test = test_data[features]\n",
    "y_test = test_data[target]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#store dso of Test data\n",
    "dbt_test_data = test_data[\"dso\"]\n",
    "dso_test_subset = Test_subset[\"dso\"]\n",
    "\n",
    "\n",
    "print(\"X_train shape: \", X_train.shape)\n",
    "print(\"X_val shape: \", X_val.shape)\n",
    "print(\"X_val_subset shape: \", X_val_subset.shape)\n",
    "print(\"Test data shape: \", X_test.shape)\n",
    "print(\"Test data subset shape: \", X_test_subset.shape)\n",
    "print(\"-------------------------\")\n",
    "print(\"Test if it worked:\") #printing the earliest and latest dates in the train and test sets\n",
    "print(\"Earliest payment date in X_train: \", X_train['weighted_payment_date'].min())\n",
    "print(\"Latest payment date in X_train: \", X_train['weighted_payment_date'].max())\n",
    "print(\"-----\")\n",
    "print(\"Earliest payment date in X_val: \", X_val['weighted_payment_date'].min())\n",
    "print(\"Latest payment date in X_val: \", X_val['weighted_payment_date'].max())\n",
    "print(\"-----\")\n",
    "print(\"Earliest payment date in X_val_subset: \", X_val_subset['weighted_payment_date'].min())\n",
    "print(\"Latest payment date in X_val_subset: \", X_val_subset['weighted_payment_date'].max())\n",
    "print(\"-----\")\n",
    "print(\"Earliest payment date in X_test: \", X_test['weighted_payment_date'].min())\n",
    "print(\"Latest payment date in X_test: \", X_test['weighted_payment_date'].max())\n",
    "print(\"-----\")\n",
    "print(\"Earliest payment date in X_test_subset: \", X_test_subset['weighted_payment_date'].min())\n",
    "print(\"Latest payment date in X_test_subset: \", X_test_subset['weighted_payment_date'].max())\n",
    "\n",
    "\n",
    "\n",
    "#drop weighted_payment_date\n",
    "X_train = X_train.drop(columns = [\"weighted_payment_date\"])\n",
    "X_val_subset = X_val_subset.drop(columns = [\"weighted_payment_date\"])\n",
    "X_test_subset = X_test_subset.drop(columns = [\"weighted_payment_date\"])\n",
    "X_test = X_test.drop(columns = [\"weighted_payment_date\"])\n",
    "\n",
    "#Drop customer id\n",
    "X_train = X_train.drop(columns = [\"customer_id\"])\n",
    "X_val_subset = X_val_subset.drop(columns = [\"customer_id\"])\n",
    "X_test_subset = X_test_subset.drop(columns = [\"customer_id\"])\n",
    "X_test = X_test.drop(columns = [\"customer_id\"])\n",
    "\n",
    "#Convert to float\n",
    "X_train = X_train.astype(float)\n",
    "y_train = y_train.astype(float)\n",
    "X_val_subset = X_val_subset.astype(float)\n",
    "y_val_subset = y_val_subset.astype(float)\n",
    "X_test_subset = X_test_subset.astype(float)\n",
    "y_test_subset = y_test_subset.astype(float)\n",
    "X_test = X_test.astype(float)\n",
    "y_test = y_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store csv files\n",
    "X_train.to_csv(f\"Inputs/Dataset_{dataset}/X_train.csv\", index=False)\n",
    "y_train.to_csv(f\"Inputs/Dataset_{dataset}/y_train.csv\", index=False)\n",
    "X_val_subset.to_csv(f\"Inputs/Dataset_{dataset}/X_val.csv\", index=False)\n",
    "y_val_subset.to_csv(f\"Inputs/Dataset_{dataset}/y_val.csv\", index=False)\n",
    "X_test_subset.to_csv(f\"Inputs/Dataset_{dataset}/X_test.csv\", index=False)\n",
    "y_test_subset.to_csv(f\"Inputs/Dataset_{dataset}/y_test.csv\", index=False)\n",
    "X_test.to_csv(f\"Inputs/Dataset_{dataset}/X_test_all_customers.csv\", index=False)\n",
    "y_test.to_csv(f\"Inputs/Dataset_{dataset}/y_test_all_customers.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
